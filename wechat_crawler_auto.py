#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¾®ä¿¡å…¬ä¼—å·è‡ªåŠ¨çˆ¬è™«
=================

è‡ªåŠ¨ç™»å½•ã€çˆ¬å–æ–‡ç« å¹¶ä¿å­˜åˆ°MySQLæ•°æ®åº“
"""

import requests
import json
import time
import random
import os
import ssl
import urllib3
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional

from spider.log.utils import logger
from auto_login import AutoLogin
from database import DatabaseManager
from get_cookie import extract_article_content_from_html

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ç¦ç”¨ä»£ç†
os.environ['NO_PROXY'] = '*'
os.environ['no_proxy'] = '*'
# æ¸…é™¤å¯èƒ½å­˜åœ¨çš„ä»£ç†è®¾ç½®
for proxy_var in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:
    if proxy_var in os.environ:
        del os.environ[proxy_var]

class WeChatCrawlerAuto:
    """å¾®ä¿¡å…¬ä¼—å·è‡ªåŠ¨çˆ¬è™«"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.auto_login = AutoLogin()
        self.db = DatabaseManager()
        self.token = None
        self.cookie_string = None
        self.headers = None
        self.accounts = []
        self.login_time = None  # è®°å½•ç™»å½•æ—¶é—´
        
    def initialize(self):
        """åˆå§‹åŒ–çˆ¬è™«ï¼ŒåŒ…æ‹¬ç™»å½•å’Œæ•°æ®åº“è¿æ¥"""
        logger.info("="*60)
        logger.info("åˆå§‹åŒ–å¾®ä¿¡å…¬ä¼—å·çˆ¬è™«")
        logger.info("="*60)
        
        # ç¡®ä¿ç™»å½•
        self.token, self.cookie_string, self.headers = self.auto_login.ensure_login()
        if not self.token:
            logger.error("æ— æ³•ç™»å½•ï¼Œçˆ¬è™«åˆå§‹åŒ–å¤±è´¥")
            return False
        self.login_time = datetime.now()  # è®°å½•ç™»å½•æ—¶é—´
        
        # åŠ è½½å…¬ä¼—å·åˆ—è¡¨
        self.accounts = self.auto_login.update_accounts_in_config()
        if not self.accounts:
            logger.error("æ²¡æœ‰è¦çˆ¬å–çš„å…¬ä¼—å·")
            return False
        
        # è¿æ¥æ•°æ®åº“
        if not self.db.connect():
            logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥")
            return False
        
        logger.success("çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
        return True
    
    def check_and_refresh_login(self) -> bool:
        """
        æ£€æŸ¥ç™»å½•çŠ¶æ€ï¼Œå¦‚æœè¶…è¿‡é…ç½®çš„å°æ—¶æ•°åˆ™è‡ªåŠ¨é‡æ–°ç™»å½•
        
        Returns:
            bool: ç™»å½•çŠ¶æ€æ˜¯å¦æœ‰æ•ˆ
        """
        if self.login_time:
            elapsed_hours = (datetime.now() - self.login_time).total_seconds() / 3600
            
            # ä»é…ç½®æ–‡ä»¶è¯»å–ç™»å½•ç¼“å­˜æ—¶é—´ï¼Œé»˜è®¤ä¸º89å°æ—¶
            config = self.auto_login.load_config()
            login_cache_hours = config.get('login_cache_hours', 89)
            
            if elapsed_hours >= login_cache_hours:
                logger.warning(f"ç™»å½•å·²è¶…è¿‡{login_cache_hours}å°æ—¶({elapsed_hours:.1f}å°æ—¶)ï¼Œå¼€å§‹è‡ªåŠ¨é‡æ–°ç™»å½•...")
                logger.error("âš ï¸âš ï¸âš ï¸ Tokenå³å°†è¿‡æœŸï¼è¯·ç«‹å³æ‰«ç é‡æ–°ç™»å½•ï¼âš ï¸âš ï¸âš ï¸")
                logger.info("â˜…â˜…â˜… è¯·åœ¨æµè§ˆå™¨ä¸­æ‰«ç é‡æ–°ç™»å½• â˜…â˜…â˜…")
                logger.info("ğŸ”” æµè§ˆå™¨çª—å£å°†ä¿æŒæ‰“å¼€çŠ¶æ€ï¼Œç›´åˆ°æ‚¨å®Œæˆç™»å½•...")
                self.token, self.cookie_string, self.headers = self.auto_login.ensure_login()
                if self.token:
                    self.login_time = datetime.now()
                    logger.success("âœ… é‡æ–°ç™»å½•æˆåŠŸï¼Œç»§ç»­çˆ¬å–")
                    return True
                else:
                    logger.error("âŒ é‡æ–°ç™»å½•å¤±è´¥")
                    return False
        return True
    
    def handle_api_error(self, response_data: dict, account_name: str = "") -> bool:
        """
        å¤„ç†APIå“åº”é”™è¯¯ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°ç™»å½•
        
        Args:
            response_data: APIå“åº”æ•°æ®
            account_name: å½“å‰å¤„ç†çš„å…¬ä¼—å·åç§°
            
        Returns:
            bool: æ˜¯å¦éœ€è¦é‡æ–°ç™»å½•å¹¶å·²æˆåŠŸé‡æ–°ç™»å½•
        """
        base_resp = response_data.get('base_resp', {}) if isinstance(response_data, dict) else {}
        ret = base_resp.get('ret', None)
        err_msg = base_resp.get('err_msg') or base_resp.get('errmsg')
        
        # æ£€æµ‹ç™»å½•å¤±æ•ˆé”™è¯¯ç 
        if ret in [200003, 200013]:  # invalid session æˆ–æœªç™»å½•
            logger.warning(f"æ£€æµ‹åˆ°ç™»å½•å¤±æ•ˆ (ret={ret}, err={err_msg})ï¼Œå°è¯•é‡æ–°ç™»å½•...")
            logger.error("âš ï¸âš ï¸âš ï¸ ç™»å½•å¤±æ•ˆï¼è¯·ç«‹å³æ‰«ç é‡æ–°ç™»å½•ï¼âš ï¸âš ï¸âš ï¸")
            logger.info("â˜…â˜…â˜… è¯·åœ¨æµè§ˆå™¨ä¸­æ‰«ç é‡æ–°ç™»å½• â˜…â˜…â˜…")
            logger.info("ğŸ”” æµè§ˆå™¨çª—å£å°†ä¿æŒæ‰“å¼€çŠ¶æ€ï¼Œç›´åˆ°æ‚¨å®Œæˆç™»å½•...")
            self.token, self.cookie_string, self.headers = self.auto_login.ensure_login()
            if self.token:
                self.login_time = datetime.now()
                logger.success("âœ… é‡æ–°ç™»å½•æˆåŠŸï¼Œç»§ç»­çˆ¬å–")
                return True
            else:
                logger.error("âŒ é‡æ–°ç™»å½•å¤±è´¥ï¼Œåœæ­¢çˆ¬å–")
                return False
        return False
    
    def search_account(self, account_name: str, retry_count: int = 0) -> Optional[str]:
        """
        æœç´¢å…¬ä¼—å·å¹¶è·å–fakeid
        
        Args:
            account_name: å…¬ä¼—å·åç§°
            retry_count: é‡è¯•æ¬¡æ•°
            
        Returns:
            str: fakeidï¼Œå¦‚æœæ‰¾ä¸åˆ°è¿”å›None
        """
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç™»å½•
        if not self.check_and_refresh_login():
            return None
            
        search_url = 'https://mp.weixin.qq.com/cgi-bin/searchbiz'
        params = {
            'action': 'search_biz',
            'token': self.token,
            'lang': 'zh_CN',
            'f': 'json',
            'ajax': '1',
            'random': random.random(),
            'query': account_name,
            'begin': 0,
            'count': '5'
        }
        
        try:
            response = requests.get(
                search_url, 
                headers=self.headers,
                params=params, 
                verify=False,
                timeout=30,
                proxies={}  # ç¦ç”¨ä»£ç†
            )
            
            if response.status_code == 200:
                data = response.json()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°ç™»å½•
                if self.handle_api_error(data, account_name) and retry_count < 1:
                    # é‡æ–°ç™»å½•æˆåŠŸï¼Œé‡è¯•æœç´¢
                    return self.search_account(account_name, retry_count + 1)
                
                lists = data.get('list', [])
                
                if lists:
                    fakeid = lists[0].get('fakeid')
                    logger.info(f"æ‰¾åˆ°å…¬ä¼—å· [{account_name}]ï¼Œfakeid: {fakeid}")
                    return fakeid
                else:
                    logger.warning(f"æœªæ‰¾åˆ°å…¬ä¼—å·: {account_name}")
            else:
                logger.error(f"æœç´¢å…¬ä¼—å·å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                
        except Exception as e:
            logger.error(f"æœç´¢å…¬ä¼—å·æ—¶å‡ºé”™: {e}")
        
        return None
    
    def get_article_content(self, url: str) -> str:
        """
        è·å–æ–‡ç« å…¨æ–‡å†…å®¹
        
        Args:
            url: æ–‡ç« URL
            
        Returns:
            str: æ–‡ç« å†…å®¹
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            }
            
            response = requests.get(url, headers=headers, timeout=30, verify=False, proxies={})
            if response.status_code == 200:
                content_data = extract_article_content_from_html(response.text)
                return content_data.get('content', 'è·å–å†…å®¹å¤±è´¥')
            else:
                logger.warning(f"è·å–æ–‡ç« å†…å®¹å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return "è·å–å†…å®¹å¤±è´¥"
                
        except Exception as e:
            logger.error(f"è·å–æ–‡ç« å†…å®¹æ—¶å‡ºé”™: {e}")
            return "è·å–å†…å®¹å¤±è´¥"
    
    def crawl_account_articles(self, account_name: str, fakeid: str, max_articles: int = 50, retry_count: int = 0) -> List[Dict]:
        """
        çˆ¬å–æŸä¸ªå…¬ä¼—å·çš„æ–‡ç« 
        
        Args:
            account_name: å…¬ä¼—å·åç§°
            fakeid: å…¬ä¼—å·çš„fakeid
            max_articles: æœ€å¤§çˆ¬å–æ–‡ç« æ•°
            retry_count: é‡è¯•æ¬¡æ•°
            
        Returns:
            List[Dict]: æ–‡ç« æ•°æ®åˆ—è¡¨
        """
        articles = []
        logger.info(f"å¼€å§‹çˆ¬å–å…¬ä¼—å·: {account_name}")
        
        # è·å–é…ç½®å‚æ•°
        config = self.auto_login.load_config()
        crawl_days = config.get('crawl_days', 7)  # åŒ…å«ä»Šå¤©è‡³å½“å‰æ—¶åˆ» + å‰ N ä¸ªå®Œæ•´è‡ªç„¶æ—¥
        article_interval = config.get('article_interval', [2, 5])  # æ–‡ç« é—´éš”æ—¶é—´
        
        # è®¡ç®—æ—¶é—´èŒƒå›´ï¼šä»ä»Šå¤©00:00å¾€å‰æ¨ N å¤©ï¼ˆå®Œæ•´è‡ªç„¶æ—¥ï¼‰ï¼Œå†åŠ ä¸Šä»Šå¤©è‡³å½“å‰æ—¶åˆ»
        now = datetime.now()
        start_of_today = now.replace(hour=0, minute=0, second=0, microsecond=0)
        start_time = start_of_today - timedelta(days=crawl_days)
        start_timestamp = int(start_time.timestamp())
        logger.info(f"å°†çˆ¬å–ä» {start_time.strftime('%Y-%m-%d %H:%M:%S')} è‡³ä»Šçš„æ–‡ç« ï¼ˆåŒ…å«ä»Šå¤©è‡³å½“å‰æ—¶é—´ + å‰ {crawl_days} ä¸ªå®Œæ•´è‡ªç„¶æ—¥ï¼‰")
        
        for begin in range(0, max_articles, 5):
            # æ¯æ‰¹æ¬¡å‰æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç™»å½•
            if not self.check_and_refresh_login():
                logger.error("ç™»å½•å¤±æ•ˆä¸”æ— æ³•é‡æ–°ç™»å½•ï¼Œåœæ­¢çˆ¬å–")
                break
                
            logger.info(f"æ­£åœ¨è·å–ç¬¬ {begin+1} åˆ° {min(begin+5, max_articles)} ç¯‡æ–‡ç« ...")
            
            # æ„å»ºè¯·æ±‚URL
            url = f'https://mp.weixin.qq.com/cgi-bin/appmsg'
            params = {
                'token': self.token,
                'lang': 'zh_CN',
                'f': 'json',
                'ajax': '1',
                'random': random.random(),
                'action': 'list_ex',
                'begin': begin,
                'count': 5,
                'query': '',
                'fakeid': fakeid,
                'type': '9'
            }
            
            try:
                response = requests.get(
                    url,
                    headers=self.headers,
                    params=params,
                    verify=False,
                    timeout=30,
                    proxies={}  # ç¦ç”¨ä»£ç†
                )
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°ç™»å½•
                    if self.handle_api_error(data, account_name) and retry_count < 1:
                        # é‡æ–°ç™»å½•æˆåŠŸï¼Œé‡æ–°å¼€å§‹çˆ¬å–è¿™ä¸ªå…¬ä¼—å·
                        return self.crawl_account_articles(account_name, fakeid, max_articles, retry_count + 1)
                    
                    app_msg_list = data.get('app_msg_list', [])
                    
                    if not app_msg_list:
                        logger.info("æ²¡æœ‰æ›´å¤šæ–‡ç« äº†")
                        break
                    
                    for item in app_msg_list:
                        # æ£€æŸ¥æ–‡ç« æ—¶é—´
                        create_time = item.get('create_time', 0)
                        if create_time < start_timestamp:
                            logger.info("æ–‡ç« å‘å¸ƒæ—¶é—´æ—©äºé…ç½®çš„èµ·å§‹æ—¶é—´ï¼Œåœæ­¢çˆ¬å–æ›´æ—©çš„æ–‡ç« ")
                            return articles  # åœæ­¢çˆ¬å–æ›´æ—©çš„æ–‡ç« 
                        
                        # æå–æ–‡ç« ä¿¡æ¯
                        article_data = {
                            'title': item.get('title', ''),
                            'url': item.get('link', ''),
                            'publish_time': create_time,
                            'digest': item.get('digest', ''),
                            'cover': item.get('cover', ''),
                            'account_name': account_name
                        }
                        
                        # è·å–æ–‡ç« å…¨æ–‡
                        if article_data['url']:
                            logger.info(f"æ­£åœ¨æ£€æŸ¥æ–‡ç« : {article_data['title']}")
                            
                            # è·å–æ˜ å°„åçš„å•ä½åç§°ï¼ˆç”¨äºæ ‡é¢˜å»é‡ï¼‰
                            unit_name = self.db.get_unit_name(account_name)
                            
                            # æ£€æŸ¥æ–‡ç« URLæ˜¯å¦å·²å­˜åœ¨
                            if self.db.check_article_exists(article_data['url']):
                                logger.info(f"æ–‡ç« URLå·²å­˜åœ¨ï¼Œè·³è¿‡: {article_data['title']}")
                                continue
                            
                            # æ£€æŸ¥æ–‡ç« æ ‡é¢˜æ˜¯å¦å·²å­˜åœ¨
                            if article_data['title'] and self.db.check_article_exists_by_title(article_data['title'], unit_name):
                                logger.info(f"æ–‡ç« æ ‡é¢˜å·²å­˜åœ¨ï¼Œè·³è¿‡: {article_data['title']} (å•ä½: {unit_name})")
                                continue
                            
                            logger.info(f"æ­£åœ¨è·å–æ–‡ç« å…¨æ–‡: {article_data['title']}")
                            
                            content = self.get_article_content(article_data['url'])
                            article_data['content'] = content
                            
                            # å®æ—¶ä¿å­˜åˆ°æ•°æ®åº“
                            if self.db.insert_article(article_data):
                                articles.append(article_data)
                                logger.success(f"æ–‡ç« å·²ä¿å­˜: {article_data['title']}")
                            
                            # ä½¿ç”¨é…ç½®çš„æ–‡ç« é—´éš”æ—¶é—´
                            delay = random.uniform(article_interval[0], article_interval[1])
                            logger.debug(f"ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
                            time.sleep(delay)
                        
                else:
                    logger.error(f"è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    break
                    
            except Exception as e:
                logger.error(f"è·å–æ–‡ç« åˆ—è¡¨æ—¶å‡ºé”™: {e}")
                break
            
            # æ‰¹æ¬¡é—´å»¶æ—¶
            time.sleep(random.uniform(3, 6))
        
        logger.info(f"å…¬ä¼—å· [{account_name}] çˆ¬å–å®Œæˆï¼Œå…±ä¿å­˜ {len(articles)} ç¯‡æ–°æ–‡ç« ")
        return articles
    
    def crawl_all_accounts(self):
        """çˆ¬å–æ‰€æœ‰å…¬ä¼—å·çš„æ–‡ç« """
        total_articles = 0
        success_accounts = 0
        
        # è·å–é…ç½®å‚æ•°
        config = self.auto_login.load_config()
        account_interval = config.get('account_interval', [10, 20])  # å…¬ä¼—å·é—´éš”æ—¶é—´
        
        logger.info(f"å¼€å§‹çˆ¬å– {len(self.accounts)} ä¸ªå…¬ä¼—å·")
        
        for i, account_name in enumerate(self.accounts, 1):
            logger.info(f"\nå¤„ç†ç¬¬ {i}/{len(self.accounts)} ä¸ªå…¬ä¼—å·: {account_name}")
            
            # æœç´¢å…¬ä¼—å·è·å–fakeid
            fakeid = self.search_account(account_name)
            if not fakeid:
                logger.warning(f"è·³è¿‡å…¬ä¼—å·: {account_name}")
                continue
            
            # çˆ¬å–æ–‡ç« 
            articles = self.crawl_account_articles(
                account_name, 
                fakeid,
                max_articles=self.auto_login.load_config().get('max_articles_per_account', 50)
            )
            
            if articles:
                total_articles += len(articles)
                success_accounts += 1
            
            # ä½¿ç”¨é…ç½®çš„å…¬ä¼—å·é—´éš”æ—¶é—´
            if i < len(self.accounts):
                delay = random.uniform(account_interval[0], account_interval[1])
                logger.info(f"ä¼‘æ¯ {delay:.1f} ç§’åç»§ç»­ä¸‹ä¸€ä¸ªå…¬ä¼—å·...")
                time.sleep(delay)
        
        logger.info("="*60)
        logger.success(f"æ‰€æœ‰å…¬ä¼—å·çˆ¬å–å®Œæˆï¼")
        logger.info(f"æˆåŠŸçˆ¬å– {success_accounts}/{len(self.accounts)} ä¸ªå…¬ä¼—å·")
        logger.info(f"å…±ä¿å­˜ {total_articles} ç¯‡æ–°æ–‡ç« åˆ°æ•°æ®åº“")
        
        # è®°å½•æœ¬è½®çˆ¬å–å®Œæˆæ—¶é—´
        if self.db.record_crawl_completion():
            logger.info("æœ¬è½®çˆ¬å–å®Œæˆæ—¶é—´å·²è®°å½•åˆ°æ•°æ®åº“")
        else:
            logger.warning("è®°å½•çˆ¬å–å®Œæˆæ—¶é—´å¤±è´¥")
        
        logger.info("="*60)
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        try:
            # åˆå§‹åŒ–
            if not self.initialize():
                return
            
            # å¼€å§‹çˆ¬å–
            self.crawl_all_accounts()
            
        except KeyboardInterrupt:
            logger.warning("ç”¨æˆ·ä¸­æ–­çˆ¬è™«")
        except Exception as e:
            logger.error(f"çˆ¬è™«è¿è¡Œå‡ºé”™: {e}")
        finally:
            # æ¸…ç†èµ„æº
            self.db.disconnect()
            logger.info("çˆ¬è™«å·²åœæ­¢")
    
    def test_single_account(self, account_name: str):
        """
        æµ‹è¯•çˆ¬å–å•ä¸ªå…¬ä¼—å·
        
        Args:
            account_name: å…¬ä¼—å·åç§°
        """
        try:
            # åˆå§‹åŒ–
            if not self.initialize():
                return
            
            logger.info(f"æµ‹è¯•çˆ¬å–å…¬ä¼—å·: {account_name}")
            
            # æœç´¢å…¬ä¼—å·
            fakeid = self.search_account(account_name)
            if not fakeid:
                logger.error(f"æ‰¾ä¸åˆ°å…¬ä¼—å·: {account_name}")
                return
            
            # çˆ¬å–æ–‡ç« ï¼ˆåªçˆ¬å–5ç¯‡ç”¨äºæµ‹è¯•ï¼‰
            articles = self.crawl_account_articles(account_name, fakeid, max_articles=5)
            
            logger.success(f"æµ‹è¯•å®Œæˆï¼Œå…±çˆ¬å– {len(articles)} ç¯‡æ–‡ç« ")
            
        finally:
            self.db.disconnect()


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    crawler = WeChatCrawlerAuto()
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == 'test' and len(sys.argv) > 2:
            # æµ‹è¯•æ¨¡å¼ï¼Œçˆ¬å–æŒ‡å®šå…¬ä¼—å·
            crawler.test_single_account(sys.argv[2])
        elif sys.argv[1] == 'help':
            print("ä½¿ç”¨æ–¹æ³•:")
            print("  python wechat_crawler_auto.py          # çˆ¬å–æ‰€æœ‰å…¬ä¼—å·")
            print("  python wechat_crawler_auto.py test å…¬ä¼—å·åç§°  # æµ‹è¯•çˆ¬å–å•ä¸ªå…¬ä¼—å·")
        else:
            print("æœªçŸ¥å‚æ•°ï¼Œä½¿ç”¨ 'help' æŸ¥çœ‹å¸®åŠ©")
    else:
        # æ­£å¸¸è¿è¡Œï¼Œçˆ¬å–æ‰€æœ‰å…¬ä¼—å·
        crawler.run()


if __name__ == "__main__":
    main()
