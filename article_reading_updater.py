#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ–‡ç« é˜…è¯»é‡æ›´æ–°å™¨
===============

å®šæ—¶æ‰«æè¿‘7å¤©æ™®æ³•æ–‡ç« ï¼Œè°ƒç”¨ç¬¬ä¸‰æ–¹APIè·å–é˜…è¯»é‡å¹¶æ›´æ–°æ•°æ®åº“
"""

import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from database import DatabaseManager
from dsf_api_client import DSFApiClient
from spider.log.utils import logger


class ArticleReadingUpdater:
    """æ–‡ç« é˜…è¯»é‡æ›´æ–°å™¨"""
    
    def __init__(self, config_file: str = "reading_updater_config.json"):
        """
        åˆå§‹åŒ–æ›´æ–°å™¨
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_file = config_file
        self.config = self._load_config()
        
        # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        self.db = DatabaseManager(
            host=self.config.get('database', {}).get('host', '127.0.0.1'),
            port=self.config.get('database', {}).get('port', 3306),
            user=self.config.get('database', {}).get('user', 'root'),
            password=self.config.get('database', {}).get('password', '123456'),
            database=self.config.get('database', {}).get('database', 'faxuan')
        )
        
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        api_config = self.config.get('api', {})
        self.api_client = DSFApiClient(
            api_key=api_config.get('key', ''),
            verify_code=api_config.get('verify_code', ''),
            base_url=api_config.get('base_url', 'https://www.dajiala.com')
        )
        
        # é…ç½®å‚æ•°
        self.days_to_check = self.config.get('days_to_check', 7)  # æ£€æŸ¥è¿‘7å¤©
        self.batch_size = self.config.get('batch_size', 50)       # æ‰¹å¤„ç†å¤§å°
        self.max_retries = self.config.get('max_retries', 3)      # æœ€å¤§é‡è¯•æ¬¡æ•°
        
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_file}")
                return config
        except FileNotFoundError:
            logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {self.config_file}")
            default_config = self._get_default_config()
            self._save_config(default_config)
            return default_config
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "database": {
                "host": "127.0.0.1",
                "port": 3306,
                "user": "root",
                "password": "123456",
                "database": "faxuan"
            },
            "api": {
                "key": "your_api_key_here",
                "verify_code": "",
                "base_url": "https://www.dajiala.com"
            },
            "days_to_check": 7,
            "batch_size": 50,
            "max_retries": 3,
            "enabled": True
        }
    
    def _save_config(self, config: Dict):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
                logger.info(f"é…ç½®æ–‡ä»¶ä¿å­˜æˆåŠŸ: {self.config_file}")
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
    
    def get_articles_need_update(self, days: int = None, only_empty: bool = False) -> List[Dict]:
        """
        è·å–éœ€è¦æ›´æ–°é˜…è¯»é‡çš„æ™®æ³•æ–‡ç« 
        
        Args:
            days: æ£€æŸ¥çš„å¤©æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
            only_empty: æ˜¯å¦åªè·å–é˜…è¯»é‡ä¸ºç©ºçš„æ–‡ç« 
            
        Returns:
            List[Dict]: éœ€è¦æ›´æ–°çš„æ–‡ç« åˆ—è¡¨
        """
        if days is None:
            days = self.days_to_check
        
        # ç¡®ä¿æ•°æ®åº“å·²è¿æ¥
        if not self.db.connection:
            logger.error("æ•°æ®åº“æœªè¿æ¥ï¼Œæ— æ³•æŸ¥è¯¢æ–‡ç« ")
            return []
            
        try:
            with self.db.connection.cursor() as cursor:
                # è®¡ç®—æ—¶é—´èŒƒå›´
                end_date = datetime.now()
                start_date = end_date - timedelta(days=days)
                
                # åŸºç¡€æŸ¥è¯¢æ¡ä»¶ï¼š
                # 1. æ˜¯æ™®æ³•æ–‡ç«  (fx_education_articles.type_class = '1')
                # 2. åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…
                # 3. æœ‰æœ‰æ•ˆçš„æ–‡ç« URL
                base_conditions = """
                WHERE ea.type_class = '1'
                  AND ar.publish_time >= %s
                  AND ar.publish_time <= %s
                  AND ar.article_url IS NOT NULL 
                  AND ar.article_url != ''
                """
                
                # å¦‚æœåªè·å–é˜…è¯»é‡ä¸ºç©ºçš„æ–‡ç« ï¼Œæ·»åŠ é¢å¤–æ¡ä»¶
                if only_empty:
                    base_conditions += """
                  AND (ar.view_count IS NULL 
                       OR ar.likes IS NULL 
                       OR ar.thumbs_count IS NULL)
                    """
                
                sql = f"""
                SELECT 
                    ar.id,
                    ar.article_id,
                    ar.article_title,
                    ar.article_url,
                    ar.publish_time,
                    ar.unit_name,
                    ar.view_count,
                    ar.likes,
                    ar.thumbs_count,
                    ea.type_class
                FROM fx_article_records ar
                INNER JOIN fx_education_articles ea ON ar.article_id = ea.article_id
                {base_conditions}
                ORDER BY ar.publish_time DESC
                """
                
                cursor.execute(sql, (start_date, end_date))
                articles = cursor.fetchall()
                
                article_type = "é˜…è¯»é‡ä¸ºç©ºçš„" if only_empty else ""
                logger.info(f"æŸ¥è¯¢åˆ° {len(articles)} ç¯‡éœ€è¦æ›´æ–°{article_type}æ™®æ³•æ–‡ç«  "
                           f"(æ—¶é—´èŒƒå›´: {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')})")
                
                return articles
                
        except Exception as e:
            logger.error(f"æŸ¥è¯¢éœ€è¦æ›´æ–°çš„æ–‡ç« æ—¶å‡ºé”™: {e}")
            return []
    
    def get_articles_for_specific_day(self, target_date: datetime) -> List[Dict]:
        """
        è·å–æŒ‡å®šæ—¥æœŸå‘å¸ƒçš„æ™®æ³•æ–‡ç« 
        
        Args:
            target_date: ç›®æ ‡æ—¥æœŸ
            
        Returns:
            List[Dict]: è¯¥æ—¥æœŸå‘å¸ƒçš„æ–‡ç« åˆ—è¡¨
        """
        # ç¡®ä¿æ•°æ®åº“å·²è¿æ¥
        if not self.db.connection:
            logger.error("æ•°æ®åº“æœªè¿æ¥ï¼Œæ— æ³•æŸ¥è¯¢æ–‡ç« ")
            return []
            
        try:
            with self.db.connection.cursor() as cursor:
                # è®¡ç®—å½“å¤©çš„å¼€å§‹å’Œç»“æŸæ—¶é—´
                day_start = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
                day_end = target_date.replace(hour=23, minute=59, second=59, microsecond=999999)
                
                # æŸ¥è¯¢æ¡ä»¶ï¼š
                # 1. æ˜¯æ™®æ³•æ–‡ç«  (fx_education_articles.type_class = '1')
                # 2. å‘å¸ƒæ—¶é—´åœ¨ç›®æ ‡æ—¥æœŸå½“å¤©
                # 3. æœ‰æœ‰æ•ˆçš„æ–‡ç« URL
                sql = """
                SELECT 
                    ar.id,
                    ar.article_id,
                    ar.article_title,
                    ar.article_url,
                    ar.publish_time,
                    ar.unit_name,
                    ar.view_count,
                    ar.likes,
                    ar.thumbs_count,
                    ea.type_class
                FROM fx_article_records ar
                INNER JOIN fx_education_articles ea ON ar.article_id = ea.article_id
                WHERE ea.type_class = '1'
                  AND ar.publish_time >= %s
                  AND ar.publish_time <= %s
                  AND ar.article_url IS NOT NULL 
                  AND ar.article_url != ''
                ORDER BY ar.publish_time DESC
                """
                
                cursor.execute(sql, (day_start, day_end))
                articles = cursor.fetchall()
                
                logger.info(f"æŸ¥è¯¢åˆ° {len(articles)} ç¯‡éœ€è¦æ›´æ–°çš„æ™®æ³•æ–‡ç«  "
                           f"(å‘å¸ƒæ—¥æœŸ: {target_date.strftime('%Y-%m-%d')})")
                
                return articles
                
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æŒ‡å®šæ—¥æœŸæ–‡ç« æ—¶å‡ºé”™: {e}")
            return []
    
    def update_article_reading_data(self, article: Dict) -> bool:
        """
        æ›´æ–°å•ç¯‡æ–‡ç« çš„é˜…è¯»é‡æ•°æ®
        
        Args:
            article: æ–‡ç« ä¿¡æ¯å­—å…¸
            
        Returns:
            bool: æ›´æ–°æˆåŠŸè¿”å›True
        """
        try:
            article_url = article['article_url']
            article_id = article['article_id']
            article_title = article.get('article_title', 'æ— æ ‡é¢˜')
            
            logger.info(f"æ›´æ–°æ–‡ç« é˜…è¯»æ•°æ®: {article_title[:50]}...")
            
            # è°ƒç”¨APIè·å–æ•°æ®
            success, stats, error = self.api_client.get_article_stats(article_url)
            
            if not success:
                logger.warning(f"è·å–æ–‡ç« æ•°æ®å¤±è´¥: {error}")
                return False
            
            # æ›´æ–°æ•°æ®åº“
            with self.db.connection.cursor() as cursor:
                sql = """
                UPDATE fx_article_records 
                SET view_count = %s,
                    likes = %s,
                    thumbs_count = %s,
                    update_time = %s
                WHERE article_id = %s
                """
                
                current_time = datetime.now()
                values = (
                    stats['read'],      # é˜…è¯»é‡ -> view_count
                    stats['looking'],   # åœ¨çœ‹é‡ -> likes
                    stats['zan'],       # ç‚¹èµé‡ -> thumbs_count
                    current_time,       # æ›´æ–°æ—¶é—´
                    article_id          # æ–‡ç« ID
                )
                
                cursor.execute(sql, values)
                self.db.connection.commit()
                
                logger.success(f"æ–‡ç« æ•°æ®æ›´æ–°æˆåŠŸ: {article_title[:50]} - "
                             f"é˜…è¯»:{stats['read']} åœ¨çœ‹:{stats['looking']} ç‚¹èµ:{stats['zan']}")
                
                return True
                
        except Exception as e:
            logger.error(f"æ›´æ–°æ–‡ç« æ•°æ®æ—¶å‡ºé”™: {e}")
            if self.db.connection:
                self.db.connection.rollback()
            return False
    
    def batch_update_articles(self, articles: List[Dict]) -> Tuple[int, int]:
        """
        æ‰¹é‡æ›´æ–°æ–‡ç« é˜…è¯»é‡æ•°æ®
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            
        Returns:
            Tuple[int, int]: (æˆåŠŸæ•°é‡, æ€»æ•°é‡)
        """
        if not articles:
            logger.info("æ²¡æœ‰éœ€è¦æ›´æ–°çš„æ–‡ç« ")
            return 0, 0
        
        total_count = len(articles)
        success_count = 0
        
        logger.info(f"å¼€å§‹æ‰¹é‡æ›´æ–° {total_count} ç¯‡æ–‡ç« çš„é˜…è¯»é‡...")
        
        for i, article in enumerate(articles, 1):
            try:
                logger.info(f"å¤„ç†è¿›åº¦: {i}/{total_count}")
                
                # æ›´æ–°æ–‡ç« æ•°æ®
                if self.update_article_reading_data(article):
                    success_count += 1
                
                # è¿›åº¦æç¤º
                if i % 10 == 0:
                    logger.info(f"å·²å¤„ç† {i}/{total_count} ç¯‡ï¼ŒæˆåŠŸ {success_count} ç¯‡")
                
            except Exception as e:
                logger.error(f"å¤„ç†ç¬¬ {i} ç¯‡æ–‡ç« æ—¶å‡ºé”™: {e}")
                continue
        
        logger.info(f"æ‰¹é‡æ›´æ–°å®Œæˆ: æˆåŠŸ {success_count}/{total_count} ç¯‡")
        return success_count, total_count
    
    def run_update(self) -> bool:
        """
        æ‰§è¡Œæ›´æ–°ä»»åŠ¡
        
        Returns:
            bool: ä»»åŠ¡æ‰§è¡ŒæˆåŠŸè¿”å›True
        """
        try:
            # æ£€æŸ¥é…ç½®
            if not self.config.get('enabled', True):
                logger.info("æ›´æ–°ä»»åŠ¡å·²ç¦ç”¨ï¼Œè·³è¿‡æ‰§è¡Œ")
                return True
            
            if not self.config.get('api', {}).get('key'):
                logger.error("APIå¯†é’¥æœªé…ç½®ï¼Œæ— æ³•æ‰§è¡Œæ›´æ–°ä»»åŠ¡")
                return False
            
            # è¿æ¥æ•°æ®åº“
            if not self.db.connect():
                logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥")
                return False
            
            start_time = datetime.now()
            logger.info("="*60)
            logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ–‡ç« é˜…è¯»é‡æ›´æ–°ä»»åŠ¡")
            logger.info("="*60)
            logger.info(f"ä»»åŠ¡å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"æ£€æŸ¥èŒƒå›´: è¿‘ {self.days_to_check} å¤©çš„æ™®æ³•æ–‡ç« ")
            
            # ç¬¬ä¸€æ­¥ï¼šè·å–è¿‘7å¤©å†…é˜…è¯»é‡ä¸ºç©ºçš„æ–‡ç« ï¼ˆåªå¡«å……ï¼‰
            logger.info("\n" + "="*60)
            logger.info("ğŸ“ ç¬¬ä¸€æ­¥ï¼šå¡«å……è¿‘7å¤©å†…é˜…è¯»é‡ä¸ºç©ºçš„æ–‡ç« ")
            logger.info("="*60)
            empty_articles = self.get_articles_need_update(only_empty=True)
            
            # ç¬¬äºŒæ­¥ï¼šè·å–å¾€å‰æ¨6å¤©é‚£ä¸€å¤©çš„æ‰€æœ‰æ–‡ç« ï¼ˆå¼ºåˆ¶æ›´æ–°ï¼‰
            six_days_ago = datetime.now() - timedelta(days=6)
            logger.info("\n" + "="*60)
            logger.info(f"ğŸ“… ç¬¬äºŒæ­¥ï¼šæ›´æ–°å¾€å‰æ¨6å¤©çš„æ–‡ç«  (å‘å¸ƒæ—¥æœŸ: {six_days_ago.strftime('%Y-%m-%d')})")
            logger.info("="*60)
            six_days_ago_articles = self.get_articles_for_specific_day(six_days_ago)
            
            # åˆå¹¶æ–‡ç« åˆ—è¡¨ï¼Œå»é‡ï¼ˆåŸºäºarticle_idï¼‰
            all_articles = empty_articles.copy() if empty_articles else []
            existing_article_ids = {article['article_id'] for article in all_articles}
            
            # æ·»åŠ å‰6å¤©çš„æ–‡ç« ï¼ˆå»é‡ï¼‰
            additional_count = 0
            for article in six_days_ago_articles:
                if article['article_id'] not in existing_article_ids:
                    all_articles.append(article)
                    existing_article_ids.add(article['article_id'])
                    additional_count += 1
            
            if not all_articles:
                logger.info("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ç« ï¼Œä»»åŠ¡å®Œæˆ")
                return True
            
            logger.info("\n" + "="*60)
            logger.info("ğŸ“Š ä»»åŠ¡æ±‡æ€»")
            logger.info("="*60)
            logger.info(f"è¿‘{self.days_to_check}å¤©é˜…è¯»é‡ä¸ºç©ºçš„æ–‡ç« (å¡«å……): {len(empty_articles)} ç¯‡")
            logger.info(f"å¾€å‰æ¨6å¤©({six_days_ago.strftime('%Y-%m-%d')})çš„æ–‡ç« (æ›´æ–°): {len(six_days_ago_articles)} ç¯‡")
            logger.info(f"å»é‡åå®é™…éœ€è¦å¤„ç†: {len(all_articles)} ç¯‡")
            logger.info(f"  - å…¶ä¸­éœ€è¦å¡«å……: {len(empty_articles)} ç¯‡")
            logger.info(f"  - å…¶ä¸­éœ€è¦æ›´æ–°: {additional_count} ç¯‡")
            logger.info("")
            
            # æ‰¹é‡æ›´æ–°
            success_count, total_count = self.batch_update_articles(all_articles)
            
            # ç»Ÿè®¡ç»“æœ
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info("="*60)
            logger.info("ğŸ“Š ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
            logger.info("="*60)
            logger.info(f"ä»»åŠ¡ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"æ‰§è¡Œè€—æ—¶: {duration}")
            logger.info(f"å¤„ç†ç»“æœ: æˆåŠŸ {success_count}/{total_count} ç¯‡")
            
            if success_count > 0:
                logger.success(f"âœ… æˆåŠŸå¤„ç†äº† {success_count} ç¯‡æ–‡ç« çš„é˜…è¯»é‡æ•°æ®")
            
            if success_count < total_count:
                failed_count = total_count - success_count
                logger.warning(f"âš ï¸  æœ‰ {failed_count} ç¯‡æ–‡ç« å¤„ç†å¤±è´¥")
            
            return True
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œæ›´æ–°ä»»åŠ¡æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            return False
            
        finally:
            # å…³é—­æ•°æ®åº“è¿æ¥
            self.db.disconnect()
    
    def get_update_statistics(self, days: int = 7) -> Dict:
        """
        è·å–æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            days: ç»Ÿè®¡çš„å¤©æ•°
            
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            if not self.db.connect():
                return {}
            
            with self.db.connection.cursor() as cursor:
                end_date = datetime.now()
                start_date = end_date - timedelta(days=days)
                
                # ç»Ÿè®¡æ™®æ³•æ–‡ç« æ€»æ•°
                sql_total = """
                SELECT COUNT(*) as total_count
                FROM fx_article_records ar
                INNER JOIN fx_education_articles ea ON ar.article_id = ea.article_id
                WHERE ea.type_class = '1'
                  AND ar.publish_time >= %s
                  AND ar.publish_time <= %s
                """
                cursor.execute(sql_total, (start_date, end_date))
                total_result = cursor.fetchone()
                total_count = total_result['total_count'] if total_result else 0
                
                # ç»Ÿè®¡å·²æœ‰é˜…è¯»é‡æ•°æ®çš„æ–‡ç« æ•°
                sql_updated = """
                SELECT COUNT(*) as updated_count
                FROM fx_article_records ar
                INNER JOIN fx_education_articles ea ON ar.article_id = ea.article_id
                WHERE ea.type_class = '1'
                  AND ar.publish_time >= %s
                  AND ar.publish_time <= %s
                  AND ar.view_count IS NOT NULL
                  AND ar.likes IS NOT NULL
                  AND ar.thumbs_count IS NOT NULL
                """
                cursor.execute(sql_updated, (start_date, end_date))
                updated_result = cursor.fetchone()
                updated_count = updated_result['updated_count'] if updated_result else 0
                
                # è®¡ç®—éœ€è¦æ›´æ–°çš„æ•°é‡
                need_update_count = total_count - updated_count
                completion_rate = (updated_count / total_count * 100) if total_count > 0 else 0
                
                stats = {
                    'total_articles': total_count,
                    'updated_articles': updated_count,
                    'need_update_articles': need_update_count,
                    'completion_rate': round(completion_rate, 2),
                    'date_range': {
                        'start_date': start_date.strftime('%Y-%m-%d'),
                        'end_date': end_date.strftime('%Y-%m-%d')
                    }
                }
                
                logger.info(f"ç»Ÿè®¡ä¿¡æ¯ ({days}å¤©): æ€»æ•°{total_count} å·²æ›´æ–°{updated_count} "
                           f"å¾…æ›´æ–°{need_update_count} å®Œæˆç‡{completion_rate:.1f}%")
                
                return stats
                
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return {}
        finally:
            self.db.disconnect()


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ–‡ç« é˜…è¯»é‡æ›´æ–°å™¨")
    parser.add_argument("--config", default="reading_updater_config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--days", type=int, help="æ£€æŸ¥çš„å¤©æ•°")
    parser.add_argument("--stats", action="store_true", help="æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯")
    parser.add_argument("--dry-run", action="store_true", help="è¯•è¿è¡Œæ¨¡å¼ï¼ˆåªæŸ¥è¯¢ä¸æ›´æ–°ï¼‰")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ›´æ–°å™¨
    updater = ArticleReadingUpdater(args.config)
    
    if args.stats:
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        days = args.days or 7
        stats = updater.get_update_statistics(days)
        if stats:
            print(f"\nğŸ“Š è¿‘{days}å¤©æ™®æ³•æ–‡ç« é˜…è¯»é‡ç»Ÿè®¡:")
            print(f"æ€»æ–‡ç« æ•°: {stats['total_articles']}")
            print(f"å·²æ›´æ–°æ•°: {stats['updated_articles']}")
            print(f"å¾…æ›´æ–°æ•°: {stats['need_update_articles']}")
            print(f"å®Œæˆç‡: {stats['completion_rate']}%")
            print(f"æ—¶é—´èŒƒå›´: {stats['date_range']['start_date']} åˆ° {stats['date_range']['end_date']}")
    elif args.dry_run:
        # è¯•è¿è¡Œæ¨¡å¼
        if not updater.db.connect():
            logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥")
            return
        
        articles = updater.get_articles_need_update(args.days)
        logger.info(f"è¯•è¿è¡Œæ¨¡å¼: æ‰¾åˆ° {len(articles)} ç¯‡éœ€è¦æ›´æ–°çš„æ–‡ç« ")
        
        for i, article in enumerate(articles[:5], 1):  # åªæ˜¾ç¤ºå‰5ç¯‡
            logger.info(f"{i}. {article['article_title'][:50]} (å‘å¸ƒæ—¶é—´: {article['publish_time']})")
        
        if len(articles) > 5:
            logger.info(f"... è¿˜æœ‰ {len(articles) - 5} ç¯‡æ–‡ç« ")
            
        updater.db.disconnect()
    else:
        # æ‰§è¡Œæ›´æ–°ä»»åŠ¡
        success = updater.run_update()
        if success:
            logger.success("æ›´æ–°ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
        else:
            logger.error("æ›´æ–°ä»»åŠ¡æ‰§è¡Œå¤±è´¥")


if __name__ == "__main__":
    main()